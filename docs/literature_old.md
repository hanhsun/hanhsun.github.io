<!-- ---
layout: page
title: Literature Review
permalink: /literature/
--- -->

My opinion/review/conclusions books/journals/papers that I recently read and a list of the ones I'm planning to read.  
It can be useful for writing my PhD dissertation or for someone planning to read one of these.  
**Note:** Not sorted in any form.

# Books

1. Nick Bostrom, **"Superintelligence: Path, Dangers, Strategies,"** Oxford University Press, 2014.

    + To be written.   
    <br>

2. Ray Kurzweil, **"How to Create a Mind: The Secret of Human Thought Revealed,"** Penguin Books, 2013.

    + To be written.   
    <br>

3. Nate Silver, **"The Signal and the Noise: Why So Many Predictions Fail--but Some Don't,"** Penguin Books, 2015.

    + To be written.   
    <br>

4. I. Goodfellow, Y. Bengio, and A. Courville, **"Deep Learning,"** MIT Press, 2016.

    + To be written.   
    <br>

5. R. S. Sutton and A. G. Barto, **"Reinforcement Learning : An Introduction,"** 2016.

    + To be written.   
    <br>


# Journals

1. J. Valasek, M. D. Tandale, and J. Rong, **“A Reinforcement Learning - Adaptive Control Architecture for Morphing,”** J. Aerosp. Comput. Information, Commun., vol. 2, no. April, pp. 174–195, 2005.

    + To be written.   
    <br>

2. J. Valasek, J. Doebbler, M. D. Tandale, and A. J. Meade, **“Improved adaptive-reinforcement learning control for morphing unmanned air vehicles,”** IEEE Trans. Syst. Man, Cybern. Part B Cybern., vol. 38, no. 4, pp. 1014–1020, 2008.

    + To be written.   
    <br>

3. Lampton, Amanda, Niksch, Adam, and Valasek, John, **"Reinforcement Learning of a Morphing Airfoil-Policy and Discrete Learning Analysis,"** Journal of Aerospace Computing, Information, and Communication, Volume 7, Number 8, August 2010, pp. 241-260.

    + To be written.   
    <br>

4. Lampton, Amanda, Niksch, Adam, and Valasek, John, **"Reinforcement Learning of Morphing Airfoils with Aerodynamic and Structural Effects,"** Journal of Aerospace Computing, Information, and Communication, Volume 6, Number 1, January 2009, pp. 30-50.

    + To be written.   
    <br>

5. J. Kober, J. A. Bagnell, and J. Peters, **“Reinforcement learning in robotics :,”** Int. J. Rob. Res., vol. 32, no. 11, pp. 1238–1274, 2015.

    + To be written.   
    <br>

6. D. M. Roijers, P. Vamplew, S. Whiteson, and R. Dazeley, **“A survey of multi-objective sequential decision-making,”** J. Artif. Intell. Res., vol. 48, pp. 67–113, 2013.

    + To be written.   
    <br>

7. I. Grondman, L. Buoniu, G. A. D. Lopes, and R. Babuška, **“A Survey of ActorCritic Reinforcement Learning: Standard and Natural Policy Gradients,”** vol. 42, no. 6, pp. 1291–1307, 2012.

    + To be written.   
    <br>

8. M. E. Taylor and P. Stone, **“Transfer Learning for Reinforcement Learning Domains : A Survey,”** J. Mach. Learn. Res., vol. 10, pp. 1633–1685, 2009.

    + To be written.   
    <br>

9. L. Busoniu, R. Babuska, B. De Schutter, and B. De Schutter, **“A comprehensive survey of multiagent reinforcement learning,”** Syst. Man, Cybern. Part C Appl. Rev., vol. 38, no. 2, pp. 156–172, 2008.

    + To be written.   
    <br>

10. J. Valasek, K. Kirkpatrick, and A. Lampton, **“Morphing Unmanned Air Vehicle Intelligent Shape and Flight Control,”** 2011, pp. 57–85.

    + In this book chapter the authors address the problem of Air Vehicle Morphing for Mission Adaptation - a large-scale, slow, change of shape that enables the same air vehicle to perform different mission profiles. Basically, the three main points to be addressed are: when to reconfigure, how to reconfigure, and learning how to do it.  
    <br>The focus of this review will be on **learning how to reconfigure**:  
    <br>The chapter builds on the same control technique, Adaptive-Reinforcement Learning Control (A-RLC), introduced by Valasek et al. in 2005. Using the A-RLC, the air vehicle uses a Reinforcement Learning (RL) scheme to learn how to achieve optimal shapes for different flight conditions and an Structured Adaptive Model Inversion (SAMI) controller handles the trajectory tracking under changes in shape.  
    <br>An RL agent was implemented to handle a morphing wing of an air-vehicle using the tabular Q-learning algorithm, discretizing both states and actions. The RL agent learns a priori how to morph the wing based on the flight condition. The states the learning agent has access to are the wing tip chord, root chord, span, and leading edge sweep angle. The goal is learn how to morph to achieve lift coefficients between 0.09 and 0.3 +/- 0.05.
    <br>
    <br>

# Papers

1. Lampton, Amanda, Niksch, Adam, and Valasek, John, **"Morphing Airfoil with Reinforcement Learning of Four Shape Changing Parameters,"** AIAA-2008-7282, Proceedings of the AIAA Guidance, Navigation, and Control Conference, Honolulu, HI, 21 August 2008.

    + To be written.   
    <br>

2. M. Tan, **“Multi-Agent Reinforcement Learning: Independent vs. Cooperative Agents,”** Proc. Tenth Int. Conf. Mach. Learn., pp. 330–337, 1993.

    + To be written.   
    <br>

3. W. Li and E. Todorov, **“Iterative linear quadratic regulator design for nonlinear biological movement systems,”** 1st Int. Conf. Informatics Control. Autom. Robot., pp. 1–8, 2004.

    + The authors present the Iterative Linear Quadratic Regulator (ILQR) method for locally-optimal feedback control
of nonlinear dynamical systems. The method iteratively linearize a nonlinear dynamical system around a trajectory and computes a locally optimal feedback control utilizing a modified version of the Linear Quadratic Regular (LQR).  
<br>You start with a target state, initial state, and a complete initial control sequence (for example, it could be all zeros). The algorithm then computes the cost of the initial control sequence (quadratic cost function) and produces an improved control sequence.
    <br>
    <br>


# arXiv

1. H. W. Lin and M. Tegmark, **“Why does deep and cheap learning work so well?,”** 2016.

    + To be written.   
    <br>

2. V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu, **“Asynchronous Methods for Deep Reinforcement Learning,”** arXiv, vol. 48, pp. 1–28, 2016.

    + To be written.   
    <br>

3. J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, **“High-Dimensional Continuous Control Using Generalized Advantage Estimation,”** arXiv, pp. 1–9, 2016.

    + To be written.   
    <br>

4. T. Zahavy, N. Ben Zrihem, and S. Mannor, **“Graying the black box: Understanding DQNs,”** arXiv, vol. 48, 2016.

    + To be written.   
    <br>

5. N. Heess, G. Wayne, D. Silver, T. Lillicrap, Y. Tassa, and T. Erez, **“Learning Continuous Control Policies by Stochastic Value Gradients,”** arXiv, pp. 1–13, 2015.

    + To be written.   
    <br>

6. T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, **“Continuous control with deep reinforcement learning,”** arXiv Prepr. arXiv1509.02971, pp. 1–14, 2015.

    + Lillicrap et al. proposes an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. It uses batch normalization (Ioffe & Szegedy, 2015) and the insights proposed by Mnih et al. on the Deep Q-Network papers: replay buffer (minimizing correlation between samples) and target networks (present a fixed target to stabilize the backup updates).  
    <br>
    <br>

7. E. Parisotto, J. L. Ba, and R. Salakhutdinov, **“Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning,”** arXiv Prepr., pp. 1–15, 2015.

    + To be written.   
    <br>

8. A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru, J. Aru, and R. Vicente, **“Multiagent Cooperation and Competition with Deep Reinforcement Learning,”** arXiv, pp. 1–12, 2015.

    + To be written.   
    <br>

9. J. Rajendran, A. Lakshminarayanan, M. M. Khapra, P. P, and B. Ravindran, **“A2T: Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources,”** 2015.

    + To be written.   
    <br>

10. V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, **“Playing Atari with Deep Reinforcement Learning,”** arXiv Prepr. arXiv {…}, pp. 1–9, 2013.

    + To be written.   
    <br>


# Extras

1. V. Mnih, K. Kavukcuoglu, D. Silver, A. a Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, **“Human-level control through deep reinforcement learning,”** Nature, vol. 518, no. 7540, pp. 529–533, 2015.

    + To be written.   
    <br>

2. P. Kormushev, S. Calinon, and D. Caldwell, **“Reinforcement Learning in Robotics: Applications and Real-World Challenges,”** Robotics, vol. 2, no. 3, pp. 122–148, 2013.

    + To be written.   
    <br>

3. L. Panait and S. Luke, **“Cooperative multi-agent learning: The state of the art,”** Auton. Agent. Multi. Agent. Syst., vol. 11, no. 3, pp. 387–434, 2005.

    + To be written.   
    <br>

4. A. Y. Ng, **“Shaping and policy search in reinforcement learning,”** ProQuest Diss. Theses, vol. 3105322, p. 155--155 , 2003.

    + To be written.   
    <br>
